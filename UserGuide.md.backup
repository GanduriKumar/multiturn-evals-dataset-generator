# User Guide â€“ Vertical-Agnostic Eval Dataset Generator

Welcome! ðŸ‘‹  
This guide is designed to help **anyone**, including beginners, understand and use the **Eval Dataset Generator**. No technical background is required.

This version of the tool is **generic**: it works for **any supported industry vertical**, such as commerce, banking, insurance, healthcare, etc. In this guide, **LLM** means **large language model**.

---

## What This Tool Does (In Simple Words)
This tool generates **conversation datasets** and **expected responses** for testing AI systems.

It helps you:
- Create **realistic user conversations** (Eval dataset) for a chosen vertical, like commerce or banking.
- Create **correct expected answers** (Golden dataset) that show how the AI *should* respond.
- Compare your AIâ€™s answers against the expected answers and see where it passes or fails.

You choose:
1. **Which vertical** you care about (e.g., Commerce vs Banking).
2. **Which workflows** inside that vertical (e.g., Returns & Refunds, Lost Card).
3. **How users behave** (friendly, confused, tricky, etc.).
4. **What conditions apply** (e.g., item out of stock, policy not allowed, high risk).

The tool generates the datasets based on those inputs.

---

## What You Will Need
Only two things:
- A web browser (Chrome, Edge, Firefox)
- The toolâ€™s URL (shared by your team)

Optional (for scoring):
- AI model outputs file (provided by your AI/engineering team)

Optional (advanced users):
- Custom config/template files per vertical

---

## What You Can Do With This Tool

### 1. Generate Evaluation Datasets
You can generate:
- **Eval dataset** â†’ What the AI will see (just the user side of the conversation).
- **Golden dataset** â†’ Expected response variants and policy decision per conversation.

### 2. Score Your AIâ€™s Performance
If your AI or LLM has answered the eval dataset, you can:
- Upload its answers
- Let the tool compare them with the golden dataset
- Get a **scored results file** to see:
  - Which conversations passed or failed
  - Where the AI broke policy
  - How good it is at following the workflow

---

## How To Use The Web App

## Step 1: Open the Web App
Open the toolâ€™s URL in your browser.

Youâ€™ll see a simple page with:
- A dropdown for **Vertical**
- Controls for workflows, behaviours, and axes (which change based on the vertical you pick)
- File upload options
- Buttons to **Generate Dataset** and **Score Run**

---

## Step 2: Select a Vertical
At the top, choose which **industry vertical** you want to work with.

Examples:
- `Commerce`
- `Banking`
- `Insurance`
- `Healthcare`

When you select a vertical, the app will **automatically load** the workflows, user behaviours, and axes that belong to that vertical.

> ðŸ’¡ Tip: Start with a vertical you know best (for you, probably *Commerce* to begin with).

---

## Step 3: Choose Workflows (What You Want To Test)
Workflows are the **business processes** you care about.

Examples:
- In **Commerce**:
  - Product Discovery & Search
  - Checkout & Payments
  - Returns, Refunds & Exchanges
  - Trust, Safety & Fraud
- In **Banking**:
  - Lost or Stolen Card
  - Loan Eligibility Check
  - Transaction Dispute

The app will show you a list of workflows for the selected vertical.  
You can pick **one or more**.

---

## Step 4: Choose User Behaviours
User behaviours describe the **style and mood** of the user. These are defined **per vertical**, but usually cover things like:

- **Happy Path** â€“ user is clear and cooperative
- **Constraint-heavy** â€“ user has strict demands (budget, policy, etc.)
- **Ambiguous** â€“ user is unclear or vague
- **Multi-turn** â€“ user gives information bit by bit
- **Corrections** â€“ user changes their mind or corrects earlier info
- **Adversarial / Trap** â€“ user tries to trick the system or bypass policy

You can toggle **any combination** of these behaviours.  
The more you enable, the more complex the conversations become.

> ðŸ’¡ Tip: Start with *Happy Path* only, then add others later.

---

## Step 5: Choose Scenario Conditions (Axes)
Axes represent **scenario conditions** that affect the conversation. They are **vertical-specific**.

Examples for **Commerce**:
- **Price sensitivity** â†’ HIGH / MED / LOW
- **Brand preference** â†’ NONE / MED / HIGH
- **Availability** â†’ AVAILABLE / LOW STOCK / OUT OF STOCK
- **Policy boundary** â†’ ALLOWED / PARTIAL / NOT ALLOWED

Examples for **Banking** (if configured):
- **Risk level** â†’ LOW / MED / HIGH
- **KYC status** â†’ COMPLETE / PARTIAL

The app will show dropdowns for each axis for your vertical.  
Choose one option for each axis.

---

## Step 6: Set Number of Conversations
Enter how many conversations you want the tool to generate **per combination**.

For example, if you set:
- 1 vertical (Commerce)
- 2 workflows
- Some behaviours
- A set of axes
- `num_samples_per_combo = 10`

The tool will generate **10 conversations for each workflow/behaviour/axes combination**.

For getting started, try `5` or `10`.

---

## Step 7: (Optional) Upload Custom Schema Files
Most users can **ignore this section**.

Advanced users can upload:
- Custom workflow definitions
- Custom behaviours
- Custom axes

If youâ€™re not sure what these are, you donâ€™t need them. The default configuration is enough to start.

---

## Step 8: Click **Generate Dataset**
When you click the button, the tool will:
- Build all the conversations for your chosen vertical + workflows + behaviours + axes
- Package them into a ZIP file containing:
  - `<dataset_id>.dataset.json` â€“ Evaluation dataset (user turns only)
  - `<dataset_id>.golden.json` â€“ Golden dataset (expected responses)
  - `manifest.json` â€“ Generation metadata

**Dataset Naming:**
The tool intelligently names your dataset based on selections:

- **All workflows + all behaviours + all axes selected:**
  - Example: `commerce-combined-1.0.0.zip`
  - Indicates comprehensive coverage

- **Partial selections:**
  - Example: `commerce-wf-returnsrefunds-bhv-happypath-scn-policy-boundary-within-policy-1.0.0.zip`
  - Shows exactly what was selected:
    - `wf-returnsrefunds` â€“ Returns & Refunds workflow
    - `bhv-happypath` â€“ Happy Path behaviour
    - `scn-policy-boundary-within-policy` â€“ Within policy scenarios
  - For very long names, a hash suffix is added for uniqueness

- **Version number:** Always ends with `-1.0.0` (or current version)

Your browser will download the ZIP automatically.

ðŸŽ‰ **You've just generated your evaluation datasets!**

---

## Step 9: Extract and Review Your Data
1. Unzip the downloaded file
2. Open the JSON files in VS Code, Notepad++, or any text editor
3. Review the structure:
   - **dataset.json** â€“ Contains `conversations` array with user messages
   - **golden.json** â€“ Contains expected responses and policy decisions
   - **manifest.json** â€“ Documents your generation parameters

---

## How To Score Your AIâ€™s Responses
After your AI/LLM (or agent) has answered the **dataset**, your engineering team can give you a file called `model_outputs.jsonl`.

You can then use the **Scoring** part of the tool.

Youâ€™ll need (legacy scoring format):
- `golden_dataset.jsonl` (legacy JSONL scoring input; not generated by this tool)
- `model_outputs.jsonl` (from the AI run)
- A name for your model (e.g., "my-model-v1")

---

## Step 1: Go to the Scoring Section
Find the section or tab labeled something like **"Score Run"**.

It will ask for:
1. Golden dataset file (`golden_dataset.jsonl`, legacy format)
2. Model outputs file (`model_outputs.jsonl`)
3. Model name / ID

---

## Step 2: Upload Files + Enter Model Name
- Upload `golden_dataset.jsonl` (legacy format)
- Upload `model_outputs.jsonl`
- Type the model name/ID (any string to identify the model)

---

## Step 3: Click **Score Run** (Evaluation Dataset)
**Purpose:** Input for your AI/chatbot to process

**Structure:**
```json
{
  "conversations": [
    {
      "conversation_id": "commerce.happy_path.policy_boundary=within_policy,...",
      "turns": [
        {
          "turn_number": 1,
          "speaker": "user",
          "utterance": "I want to return my shoes"
        },
        {
          "turn_number": 3,
          "speaker": "user",
          "utterance": "I bought them 2 weeks ago, order #12345"
        }
      ],
      "metadata": {
        "domain_label": "Returns & Refunds",
        "behavior": "happy_path",
        "axes": {...},
        "policy_excerpt": "...",
        "facts_bullets": [...],
        "short_description": "..."
      }
    }
  ]
}
```

**Contains:**
- User turns only (agent turns removed for testing)
- Conversation metadata (domain, behavior, scenario conditions)
- Policy context and business facts

### `<dataset_id>.golden.json` (Golden Dataset)
**Purpose:** Ground truth for evaluation and scoring

**Structure:**
```Common Use Cases

### Use Case 1: First-Time Testing
**Goal:** Validate your AI works for basic scenarios

**Setup:**
- **Vertical:** Commerce
- **Workflows:** Returns & Refunds (1 workflow)
- **Behaviours:** Happy Path only
- **Axes:** Within policy, Available stock
- **Samples:** 5-10 conversations

**Why:** Establishes baseline performance before adding complexity.

---

### Use Case 2: Policy Compliance Testing
**Goal:** Ensure AI respects business policies

**Setup:**
- **Vertical:** Commerce or Insurance
- **Workflows:** Claims, Returns, Refunds
- **Behaviours:** Happy Path + Adversarial
- **Axes:** Policy Boundary = Not Allowed, Partial, Within Policy
- **Samples:** 20+ per combination

**Why:** Tests if AI correctly rejects disallowed requests and enforces rules.

---

### Use Case 3: Multi-Behavior Stress Testing
**Goal:** Test AI robustness across difficult conversations

**Setup:**
- **Vertical:** Any
- **Workflows:** 2-3 workflows
- **Behaviours:** All enabled (Happy Path, Ambiguous, Corrections, Adversarial)
- **Axes:** Mix of easy and hard scenarios
- **Samples:** 50+ conversations

**Why:** Simulates real-world user diversity and edge cases.

---

### Use Case 4: Vertical-Wide Comprehensive Testing
**Goal:** Full coverage for production readiness

**Setup:**
- **Vertical:** Your production vertical
- **Workflows:** ALL workflows
- **Behaviours:** ALL behaviours
- **Axes:** ALL combinations
- **Samples:** 100+ conversations
- **Result:** Dataset named `<vertical>-combined-1.0.0`

**Why:** Complete test suite for regression testing and benchmarking.

---

### Use Case 5: Specific Scenario Deep Dive
**Goal:** Debug failures in specific situations

**Setup:**
- **Vertical:** Banking
- **Workflows:** Lost Card (1 workflow)
- **Behaviours:** Ambiguous + Multi-Turn
- **Axes:** Risk Level = High, KYC Status = Partial
- **Samples:** 20 conversations

**Why:** Focused testing on problematic scenario combinations.

---

## Best Practices

### Strategy 1: Start Small, Scale Gradually
- Begin with 5-10 conversations, single workflow, happy path
- Review outputs carefully
- Incrementally add behaviours and scenarios
- Build to comprehensive coverage over time

### Strategy 2: Use Descriptive Names
- Partial selections create descriptive dataset names
- Makes it easy to identify what each dataset tests
- Good for organizing test suites: `wf-returns-bhv-adversarial-scn-notallowed`

### Strategy 3: Mix Behaviors for Realism
- Real users aren't always cooperative
- Include Ambiguous + Corrections for realistic difficulty
- Add Adversarial sparingly (targets edge cases)

### Strategy 4: Test Policy Boundaries Explicitly
- Most AI failures happen at policy edges
- Always test: Within Policy, Partial, Not Allowed
- Combine with Adversarial behavior for thorough validation

### Strategy 5: Version Your Datasets
- Keep track of which dataset version you're testing
- Note the version number in dataset names (`1.0.0`)
- Maintain a test log linking datasets to AI model versions

### Strategy 6: Review Manifest Files
- Check `manifest.json` to confirm generation parameters
- Use it to reproduce datasets later
- Share with team for transparency

### Strategy 7: Organized File Management
- Create folders by vertical: `datasets/commerce/`, `datasets/banking/`
- Subfolder by test type: `baseline/`, `policy-tests/`, `stress-tests/`
- Keep golden and eval datasets together

---

## Tips For Beginners

**Week 1: Familiarization**
- Start with ONE vertical you know well
- ONE workflow (e.g., Returns & Refunds)
- Happy Path behaviour only
- Simple axes (ALLOWED policy, AVAILABLE stock)
- Generate exactly **5** conversations
- Open JSON files to understand structure

**Week 2: Experimentation**
- Try 2-3 workflows
- Add Ambiguous or Multi-Turn behaviour
- Test different axes values
- Generate 10-20 conversations
- Work with engineering to run your first eval

**Week 3: Expansion**
- Enable multiple behaviours
- Test policy edge cases (Not Allowed scenarios)
- Generate 50+ conversations
- Run scoring to identify weaknesses

**Week 4: Production Ready**
- Build comprehensive test suite
- All workflows for your vertical
- Mix of behaviors
- Full axes coverage
- 100+ conversations for regression testing
      "conversation_id": "commerce.happy_path.policy_boundary=within_policy,...",
      "expected_policy_decision": "approved",
      "turns": [
        {
          "turn_number": 2,
          "expected_response_variants": [
            "I can help you with that return...",
            "I'll process that return for you..."
          ],
          "expected_actions": ["initiate_return"],
          "expected_facts": {
            "return_window": "30 days",
            "refund_method": "original payment"
          }
        }
      ]
    }
  ]
}
```

**Contains:**
- Expected response variations
- Expected actions the agent should take
- Expected facts to communicate
- Policy decision (approved/rejected/partial)

### `model_outputs.jsonl` (AI Responses)
**Purpose:** Your AI's actual responses (created outside this tool)

**Format:** JSONL (one JSON object per line)
```jsonl
{"conversation_id": "...", "agent_response": "...", "turn_number": 2}
{"conversation_id": "...", "agent_response": "...", "turn_number": 4}
```

### `scored_results.jsonl` (Scoring Output)
**Purpose:** Comparison results between golden and model outputs

**Contains:**
- Per-conversation scores
- Pass/fail status
- Policy violation flags
- Detailed error analysis

### `manifest.json` (Generation Metadata)
**Purpose:** Documentation of generation parameters

**Contains:**
- Vertical and workflows selected
- Behaviours and axes chosen
- Number of conversations generated
- Timestamp and version
- Configuration snapshot
Your data science or QA team can also turn this into dashboards.

---

## What The Files Mean

### `<dataset_id>.dataset.json`
- JSON file with a `conversations` array (user turns only).
- Used as **input** to your AI.

### `<dataset_id>.golden.json`
- Expected response variants and policy decision per conversation.
- Used as **ground truth** for evaluation pipelines that understand this format.

### `model_outputs.jsonl`
- Actual responses from your AI or agent.
- Produced **outside** this tool.

### `scored_results.jsonl`
- Per-conversation scores from comparing model outputs to golden.

### `manifest.json`
- Metadata about how the dataset was generated (vertical, workflows, behaviours, axes, counts, etc.).

---

## Tips For Beginners

- Start with:
  - One **vertical** (e.g., Commerce)
  - One **workflow** (e.g., Returns & Refunds)
  - Just **Happy Path** behaviour
  - Simple axes (e.g., ALLOWED policy, AVAILABLE stock)
- Generate only **5** conversations at first.
- Open the JSON files in VS Code or another text editor to get a feel.
- Work with your AI/engineering partner to run the eval dataset.
- Use scoring to see where the model is weak.
- Slowly add more behaviours and harder axes (like Adversarial + NOT ALLOWED policy) to stress-test your AI.

---

## Troubleshooting

### I don't understand JSONL
JSONL is a text file where **each line is a separate JSON object**. This tool only requires JSONL for the legacy `/score-run` endpoint.
- You can open it in:
  - VS Code
  - Notepad
  - Any text editor

### The AI is failing everything
Try making scenarios easier:
- Use **Happy Path** behaviour only.
- Choose policy axes like **ALLOWED** instead of NOT ALLOWED.
- Use **AVAILABLE** instead of OUT OF STOCK.

### Can I use my own conversation templates and rules
Yes. The app supports **custom schema/config uploads** per vertical.  
This is an advanced feature â€“ ask your engineering partner to help configure it.

### The download isn't working
Check:
- Browser pop-up or download permissions
- Network/firewall rules
- That the request didnâ€™t fail with an error (ask engineering to check logs if needed)

---

## You're Ready
With this guide, you can now:
- Generate datasets for different verticals
- Simulate real-world business scenarios
- Score your AIâ€™s performance
- Help your organization deploy safer, smarter AI

If you ever feel stuck, revisit this guide or reach out to your engineering/data science partner.

You can start testing.
